{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xoo9zfb99yQb"
   },
   "source": [
    "[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/lorenzobasile/DeepLearning2022/blob/main/solution_ff.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pKt5sLQO9zgn"
   },
   "source": [
    "# Assignment 3 - Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Jo434asKrS0",
    "outputId": "9b5ebc52-aaed-46fd-91ca-297a194ad80e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n",
      "/content/drive/MyDrive/DeepLearning2022\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from google.colab import drive\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "%cd drive/MyDrive/DeepLearning2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kfbM76wz9_kg"
   },
   "source": [
    "# The Forward Forward algorithm\n",
    "\n",
    "The [Forward Forward algorithm](https://www.cs.toronto.edu/~hinton/FFA13.pdf) has been presented by Geoffrey Hinton (one of the \"Deep Learning godfathers\") at NeurIPS 2022, less than a month ago.\n",
    "\n",
    "It is a novel technique for Neural Network optimization, alternative to standard Backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ztmvsf2WB94X"
   },
   "source": [
    "## An alternative to Backpropagation\n",
    "\n",
    "Over the last decades, backprop has been so successful that neuroscientists have been looking for proofs that the brain actually learns in a similar way. However, that does not seem to happen. Moreover, backprop encounters serious limitations if the model is not exactly defined in mathematical terms: if the forward pass is a black box, there is no possibility to make a backward pass (as we don't know what to differentiate).\n",
    "\n",
    "Even if (for the moment) it was not presented with the goal of substituting backprop for practical applications, the Forward Forward algorithm aims at solving some of these problems, using a layer-by-layer learning approach that does not require backward propagation of gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-42jTD10GapS"
   },
   "source": [
    "## MNIST classification through FF\n",
    "\n",
    "Let's see how FF works through a practical example on MNIST classification.\n",
    "\n",
    "Citing the paper,  \"the idea is to replace the forward and backward passes of backpropagation by two forward\n",
    "passes that operate in exactly the same way as each other, but on different data and with opposite\n",
    "objectives. The positive pass operates on real data and adjusts the weights to increase the goodness in\n",
    "every hidden layer. The negative pass operates on \"negative data\" and adjusts the weights to decrease\n",
    "the goodness in every hidden layer. This paper explores two different measures of goodness â€“ the\n",
    "sum of the squared neural activities and the negative sum of the squared activities, but many other\n",
    "measures are possible.\"\n",
    "\n",
    "Thus, we have to find a way to define \"positive\" and \"negative\" data. If we want to classify MNIST, a data point being positive means that it somehow contains the right assignment of an image to its corresponding class. Hence, we can define a positive datapoint by embedding the right classification into the image and a negative datapoint by embedding a random classification. To do so, we can exploit the fact that there is a black frame around the digit, and we can use the top 10 pixels on the left to embed class information.\n",
    "\n",
    "![](images/ff_mnist.png)\n",
    "\n",
    "Then, the layers of the network are trained one at a time with the objective of assigning high activation values (high goodness) to positive points and low to negative ones. However, if there are many layers, it would be trivial for following layers to rely on the information coming from the previous ones (if the activations of layer $i$ are high, this drastically increases the probability that the activations of layer $i+1$ are high as well). To avoid this phenomenon and make layers learn different features, it is necessary to normalize the activation vector to have norm $1$.\n",
    "\n",
    "At the end, at evaluation time, one can query the network with a certain image with all possible labels, and take the argmax of the goodnesses to obtain the prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "zeyKi7Sj1gcn"
   },
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, dimensions):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.ModuleList([ReLULayer(dimensions[i], dimensions[i + 1]) for i in range(len(dimensions)-1)])\n",
    "    def predict(self, x):\n",
    "        goodness_per_label = []\n",
    "        for label in range(10):\n",
    "            x_lab = label_images(x, label)\n",
    "            goodness = []\n",
    "            for i, layer in enumerate(self.layers):\n",
    "                x_lab = layer(x_lab)\n",
    "                if i>0:\n",
    "                    goodness.append(torch.mean(x_lab.pow(2), dim=1))\n",
    "            goodness_per_label.append(sum(goodness).unsqueeze(1))\n",
    "        goodness_per_label = torch.cat(goodness_per_label, 1)\n",
    "        return torch.argmax(goodness_per_label, dim=1)\n",
    "    \n",
    "    def train(self, x_pos, x_neg):\n",
    "        for layer in self.layers:\n",
    "            x_pos, x_neg = layer.train(x_pos, x_neg)\n",
    "    \n",
    "    def alternated_train(self, x_pos, x_neg, cycles=1000): # cycles=1000 means that each day/night lasts for 1 epoch (num_epochs//cycles==1)\n",
    "        for layer in self.layers:\n",
    "            for cycle in range(cycles):\n",
    "                layer.day(x_pos, cycles)\n",
    "                layer.night(x_neg, cycles)\n",
    "            x_pos, x_neg = layer(x_pos).detach(), layer(x_neg).detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "zhuC5SwH1iCz"
   },
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return x / (x.norm(p=2, dim=1).reshape(-1,1))\n",
    "\n",
    "class ReLULayer(torch.nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = torch.nn.Linear(in_features, out_features)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.optimizer = torch.optim.Adam(self.parameters(), lr=0.03)\n",
    "        self.threshold = 2.0\n",
    "        self.num_epochs = 1000\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_direction = normalize(x)\n",
    "        return self.relu(self.linear(x_direction))\n",
    "\n",
    "    def train(self, x_pos, x_neg):\n",
    "        for i in range(self.num_epochs):\n",
    "            positive_goodness = torch.mean(self.forward(x_pos).pow(2), dim=1)\n",
    "            negative_goodness = torch.mean(self.forward(x_neg).pow(2), dim=1)\n",
    "            l = torch.log(1 + torch.exp(torch.cat([\n",
    "                -positive_goodness + self.threshold,\n",
    "                negative_goodness - self.threshold]))).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "        return self.forward(x_pos).detach(), self.forward(x_neg).detach()\n",
    "    \n",
    "    def day(self, x_pos, cycles):\n",
    "        for i in range(self.num_epochs//cycles):\n",
    "            positive_goodness = torch.mean(self.forward(x_pos).pow(2), dim=1)\n",
    "            l = torch.log(1 + torch.exp(-positive_goodness + self.threshold)).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "        return\n",
    "    \n",
    "    def night(self, x_neg, cycles):\n",
    "        for i in range(self.num_epochs//cycles):\n",
    "            negative_goodness = torch.mean(self.forward(x_neg).pow(2), dim=1)\n",
    "            l = torch.log(1 + torch.exp(negative_goodness - self.threshold)).mean()\n",
    "            self.optimizer.zero_grad()\n",
    "            l.backward()\n",
    "            self.optimizer.step()\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "a1dSgF_m1jW6",
    "outputId": "62cc59a9-3bc2-4256-ad35-9addb0e6ed21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train accuracy: 0.8846499919891357\n",
      "Test accuracy: 0.890999972820282\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor(), torchvision.transforms.Normalize((0.1307,), (0.3081,)), torchvision.transforms.Lambda(torch.flatten)])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST('./data/', transform=transform,  train=True, download=True)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=60000, shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST('./data/', transform=transform, train=False, download=True)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=10000, shuffle=False)\n",
    "\n",
    "def label_images(images, labels):\n",
    "    labeled_images = images.clone()\n",
    "    labeled_images[range(len(images)), labels]=images.max()\n",
    "    return labeled_images\n",
    "\n",
    "net = Net([784, 500, 500]).to(device)\n",
    "x, y = next(iter(trainloader))\n",
    "x=x.to(device)\n",
    "y=y.to(device)\n",
    "x_pos = label_images(x, y)\n",
    "rnd = torch.randperm(x.size(0))\n",
    "\n",
    "x_neg = label_images(x, y[rnd])\n",
    "\n",
    "#net.train(x_pos, x_neg)\n",
    "net.alternated_train(x_pos, x_neg)\n",
    "\n",
    "print('Train accuracy:', net.predict(x).eq(y).float().mean().item())\n",
    "\n",
    "x_te, y_te = next(iter(testloader))\n",
    "x_te=x_te.to(device)\n",
    "y_te=y_te.to(device)\n",
    "\n",
    "print('Test accuracy:', net.predict(x_te).eq(y_te).float().mean().item())"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
